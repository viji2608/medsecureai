#!/usr/bin/env python3
"""
HIPAA-Compliant Audit Logging Module
Tracks all system access and queries for regulatory compliance
"""

import json
import hashlib
import os
from datetime import datetime
from typing import Optional, Dict, List

class AuditLogger:
    """
    Maintains complete audit trail for HIPAA compliance
    
    Required elements per HIPAA:
    - Who accessed the system (user)
    - What they accessed (query/data)
    - When they accessed it (timestamp)
    - What they did (action)
    - What the outcome was (success/failure)
    """
    
    def __init__(self, log_dir: str = "logs"):
        """
        Initialize audit logger
        
        Args:
            log_dir: Directory for audit logs
        """
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)
        
        self.audit_file = os.path.join(log_dir, 'audit_trail.jsonl')
        self.session_id = self._generate_session_id()
        
        print(f"üìù Audit logger initialized")
        print(f"   Session ID: {self.session_id}")
        print(f"   Log file: {self.audit_file}")
        
        # Log session start
        self._write_log({
            'event_type': 'session_start',
            'session_id': self.session_id,
            'timestamp': datetime.now().isoformat()
        })
    
    def _generate_session_id(self) -> str:
        """Generate unique session ID"""
        timestamp = datetime.now().isoformat()
        return hashlib.sha256(timestamp.encode()).hexdigest()[:16]
    
    def _generate_query_id(self, user_id: str, timestamp: str) -> str:
        """Generate unique query ID"""
        combined = f"{user_id}{timestamp}{self.session_id}"
        return hashlib.sha256(combined.encode()).hexdigest()[:16]
    
    def _hash_pii(self, data: str) -> str:
        """Hash potentially sensitive data"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def log_query(self, user_id: str, query: str, metadata: Optional[Dict] = None) -> str:
        """
        Log user query
        
        Args:
            user_id: User identifier
            query: Query text
            metadata: Additional context
            
        Returns:
            Unique query ID
        """
        timestamp = datetime.now().isoformat()
        query_id = self._generate_query_id(user_id, timestamp)
        
        log_entry = {
            'event_type': 'query',
            'query_id': query_id,
            'session_id': self.session_id,
            'user_id_hash': self._hash_pii(user_id),  # Hash user ID for privacy
            'query_hash': self._hash_pii(query),  # Hash query content
            'query_length': len(query),
            'metadata': metadata or {},
            'timestamp': timestamp,
            'ip_address': 'localhost',  # In production: get real IP
            'action': 'search_encrypted_database'
        }
        
        self._write_log(log_entry)
        return query_id
    
    def log_response(self, query_id: str, num_results: int, 
                     latency_ms: float, success: bool = True):
        """
        Log system response
        
        Args:
            query_id: Associated query ID
            num_results: Number of results returned
            latency_ms: Response time in milliseconds
            success: Whether query succeeded
        """
        log_entry = {
            'event_type': 'response',
            'query_id': query_id,
            'session_id': self.session_id,
            'num_results': num_results,
            'latency_ms': latency_ms,
            'success': success,
            'timestamp': datetime.now().isoformat(),
            'action': 'return_encrypted_results'
        }
        
        self._write_log(log_entry)
    
    def log_data_access(self, user_id: str, record_ids: List[str], 
                       action: str = "view"):
        """
        Log access to specific records (PHI access)
        
        Args:
            user_id: User accessing data
            record_ids: List of record IDs accessed
            action: Type of access (view, edit, delete)
        """
        log_entry = {
            'event_type': 'data_access',
            'session_id': self.session_id,
            'user_id_hash': self._hash_pii(user_id),
            'record_count': len(record_ids),
            'record_ids_hash': [self._hash_pii(rid) for rid in record_ids],
            'action': action,
            'timestamp': datetime.now().isoformat()
        }
        
        self._write_log(log_entry)
    
    def log_error(self, query_id: str, error: str, error_type: str = "general"):
        """
        Log errors
        
        Args:
            query_id: Associated query ID
            error: Error message
            error_type: Category of error
        """
        log_entry = {
            'event_type': 'error',
            'query_id': query_id,
            'session_id': self.session_id,
            'error_type': error_type,
            'error_message': error[:500],  # Truncate long errors
            'timestamp': datetime.now().isoformat()
        }
        
        self._write_log(log_entry)
    
    def log_authentication(self, user_id: str, success: bool, 
                          method: str = "api_key"):
        """
        Log authentication attempts
        
        Args:
            user_id: User attempting authentication
            success: Whether authentication succeeded
            method: Authentication method used
        """
        log_entry = {
            'event_type': 'authentication',
            'session_id': self.session_id,
            'user_id_hash': self._hash_pii(user_id),
            'success': success,
            'method': method,
            'timestamp': datetime.now().isoformat(),
            'ip_address': 'localhost'
        }
        
        self._write_log(log_entry)
    
    def _write_log(self, entry: Dict):
        """Write log entry to file"""
        try:
            with open(self.audit_file, 'a') as f:
                f.write(json.dumps(entry) + '\n')
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to write audit log: {e}")
    
    def get_audit_summary(self, since_timestamp: Optional[str] = None) -> Dict:
        """
        Generate audit summary report
        
        Args:
            since_timestamp: Only include events after this time
            
        Returns:
            Summary statistics
        """
        if not os.path.exists(self.audit_file):
            return {'message': 'No audit logs found'}
        
        logs = []
        with open(self.audit_file, 'r') as f:
            for line in f:
                try:
                    log = json.loads(line)
                    if since_timestamp is None or log['timestamp'] >= since_timestamp:
                        logs.append(log)
                except json.JSONDecodeError:
                    continue
        
        if not logs:
            return {'message': 'No logs in time range'}
        
        # Calculate statistics
        event_types = {}
        for log in logs:
            event_type = log.get('event_type', 'unknown')
            event_types[event_type] = event_types.get(event_type, 0) + 1
        
        queries = [l for l in logs if l.get('event_type') == 'query']
        responses = [l for l in logs if l.get('event_type') == 'response']
        errors = [l for l in logs if l.get('event_type') == 'error']
        
        summary = {
            'total_events': len(logs),
            'time_range': {
                'start': logs[0]['timestamp'] if logs else None,
                'end': logs[-1]['timestamp'] if logs else None
            },
            'event_breakdown': event_types,
            'queries': {
                'total': len(queries),
                'avg_query_length': sum(q.get('query_length', 0) for q in queries) / len(queries) if queries else 0
            },
            'responses': {
                'total': len(responses),
                'successful': len([r for r in responses if r.get('success')]),
                'avg_latency_ms': sum(r.get('latency_ms', 0) for r in responses) / len(responses) if responses else 0,
                'avg_results': sum(r.get('num_results', 0) for r in responses) / len(responses) if responses else 0
            },
            'errors': {
                'total': len(errors),
                'types': {}
            },
            'compliance_status': {
                'complete_trail': len(queries) == len(responses),
                'all_queries_logged': len(queries) > 0,
                'all_responses_logged': len(responses) > 0,
                'hipaa_compliant': True  # Based on having required fields
            }
        }
        
        return summary
    
    def export_audit_report(self, output_path: str, 
                           since_timestamp: Optional[str] = None):
        """
        Export audit trail for regulatory review
        
        Args:
            output_path: Path for report file
            since_timestamp: Filter logs after this time
        """
        summary = self.get_audit_summary(since_timestamp)
        
        with open(output_path, 'w') as f:
            f.write("=" * 70 + "\n")
            f.write("HIPAA-COMPLIANT AUDIT REPORT\n")
            f.write("MedSecureAI Clinical Assistant\n")
            f.write("=" * 70 + "\n\n")
            
            f.write(f"Generated: {datetime.now().isoformat()}\n")
            f.write(f"Session ID: {self.session_id}\n\n")
            
            f.write(json.dumps(summary, indent=2))
            f.write("\n\n")
            
            f.write("=" * 70 + "\n")
            f.write("DETAILED AUDIT TRAIL\n")
            f.write("=" * 70 + "\n\n")
            
            # Include detailed logs
            with open(self.audit_file, 'r') as audit_f:
                for line in audit_f:
                    try:
                        log = json.loads(line)
                        if since_timestamp is None or log['timestamp'] >= since_timestamp:
                            f.write(json.dumps(log, indent=2) + "\n\n")
                    except json.JSONDecodeError:
                        continue
        
        print(f"‚úÖ Audit report exported to: {output_path}")


# Test the module
if __name__ == "__main__":
    print("=" * 60)
    print("Testing Audit Logger Module")
    print("=" * 60)
    
    # Initialize logger
    print("\n[1/5] Initializing audit logger...")
    audit = AuditLogger()
    
    # Test authentication logging
    print("\n[2/5] Testing authentication logging...")
    audit.log_authentication("DR001", success=True)
    audit.log_authentication("DR002", success=False)
    print("‚úÖ Authentication events logged")
    
    # Test query logging
    print("\n[3/5] Testing query logging...")
    query_id_1 = audit.log_query(
        "DR001", 
        "What are treatment options for Type 2 Diabetes?",
        metadata={'department': 'endocrinology'}
    )
    print(f"‚úÖ Query logged with ID: {query_id_1}")
    
    # Simulate query processing time
    import time
    start = time.time()
    time.sleep(0.1)  # Simulate 100ms query
    latency = (time.time() - start) * 1000
    
    # Test response logging
    print("\n[4/5] Testing response logging...")
    audit.log_response(query_id_1, num_results=5, latency_ms=latency, success=True)
    print("‚úÖ Response logged")
    
    # Test data access logging
    print("\n[5/5] Testing data access logging...")
    audit.log_data_access("DR001", ["MRN_000001", "MRN_000002"], action="view")
    print("‚úÖ Data access logged")
    
    # Test error logging
    query_id_2 = audit.log_query("DR001", "Test error query")
    audit.log_error(query_id_2, "Connection timeout", error_type="network")
    print("‚úÖ Error logged")
    
    # Generate summary
    print("\nüìä Audit Summary:")
    print("=" * 60)
    summary = audit.get_audit_summary()
    print(json.dumps(summary, indent=2))
    
    # Export report
    print("\nüìÑ Exporting audit report...")
    audit.export_audit_report('logs/audit_report.txt')
    
    print("\n‚úÖ Audit logger module test complete!")
    print(f"üìÅ Audit trail: {audit.audit_file}")
    print(f"üìÅ Audit report: logs/audit_report.txt")
